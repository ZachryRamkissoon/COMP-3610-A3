{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33b22d5",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2960de3",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e67124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26615eb",
   "metadata": {},
   "source": [
    "# Cleaning in Batches\n",
    "This section processes the merged dataset for a specific category (eg. Magazine_Subscriptions) in chunks to handle the large-scale dataset efficiently. The code loads the input Parquet file, filters out columns, and applies cleaning steps including dropping invalid/missing ratings and empty reviews, extracting brand information, removing duplicates, and deriving new columns (review length and year). Each cleaned chunk is saved as a separate Parquet file to manage memory efficiently since the dataset included some large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing files in C:\\Users\\zachr\\Magazine_Subscriptions\n",
      "âœ… Processed and saved chunk 1: 19,940 rows to C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions1.parquet\n",
      "âœ… Processed and saved chunk 2: 19,916 rows to C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions2.parquet\n",
      "âœ… Processed and saved chunk 3: 19,846 rows to C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions3.parquet\n",
      "âœ… Processed and saved chunk 4: 11,224 rows to C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions4.parquet\n",
      "\n",
      " Cleaning complete! 4 files saved to C:\\Users\\zachr\\Magazine_Subscriptions\n",
      "Total rows processed: 70,926\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "input_file = Path(\"C:/Users/zachr/Downloads/merged_Magazine_Subscriptions.parquet\")\n",
    "output_dir = Path(\"C:/Users/zachr/Magazine_Subscriptions\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if input file exists\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"Input file {input_file} does not exist\")\n",
    "\n",
    "# Clear existing cleaned files to start fresh\n",
    "for file in output_dir.glob(\"cleaned__Magazine_Subscriptions*.parquet\"):\n",
    "    os.remove(file)\n",
    "print(f\"Cleared existing files in {output_dir}\")\n",
    "\n",
    "# Create a dataset for streaming\n",
    "dataset = ds.dataset(input_file, format=\"parquet\")\n",
    "\n",
    "# Specify columns to keep\n",
    "columns_to_keep = [\n",
    "    \"parent_asin\", \"rating\", \"text\", \"details\", \"store\", \"user_id\", \"asin\", \n",
    "    \"timestamp\", \"categories\", \"main_category\", \"helpful_vote\", \n",
    "    \"verified_purchase\", \"title_y\", \"average_rating\", \"rating_number\", \"price\"\n",
    "]\n",
    "\n",
    "# Scanner for chunked reading\n",
    "scanner = dataset.scanner(columns=columns_to_keep, batch_size=50_000)\n",
    "\n",
    "# Define brand extraction logic\n",
    "def extract_brand(details, store):\n",
    "    if isinstance(details, dict):\n",
    "        brand = details.get(\"brand\")\n",
    "        if brand:\n",
    "            return brand.strip()\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store.strip()\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Process in chunks\n",
    "total_rows_processed = 0\n",
    "chunk_index = 1\n",
    "\n",
    "for i, batch in enumerate(scanner.to_batches()):\n",
    "    # Convert batch to Pandas DataFrame\n",
    "    df = batch.to_pandas()\n",
    "\n",
    "    # Drop invalid/missing ratings\n",
    "    df = df[df[\"rating\"].isin([1, 2, 3, 4, 5])]\n",
    "\n",
    "    # Drop empty/null review texts\n",
    "    df = df[df[\"text\"].notnull() & (df[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    # Extract brand\n",
    "    df[\"brand\"] = df.apply(lambda row: extract_brand(row[\"details\"], row[\"store\"]), axis=1)\n",
    "\n",
    "    # Drop details/store after extracting brand\n",
    "    df.drop(columns=[\"details\", \"store\"], inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    # Derived column: review length\n",
    "    df[\"review_length\"] = df[\"text\"].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "    # Derived column: year from timestamp\n",
    "    df[\"year\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    # Write cleaned chunk to a separate Parquet file\n",
    "    output_file = output_dir / f\"Magazine_Subscriptions{chunk_index}.parquet\"\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    pq.write_table(table, output_file, compression='snappy')\n",
    "    \n",
    "    total_rows_processed += len(df)\n",
    "    print(f\"âœ… Processed and saved chunk {chunk_index}: {len(df):,} rows to {output_file}\")\n",
    "\n",
    "    # Increment chunk index\n",
    "    chunk_index += 1\n",
    "    \n",
    "    # Clean up memory\n",
    "    del df\n",
    "    del table\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n Cleaning complete! {chunk_index - 1} files saved to {output_dir}\")\n",
    "print(f\"Total rows processed: {total_rows_processed:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43983e18",
   "metadata": {},
   "source": [
    "# Combining Cleaned Parquet Files\n",
    "This section combines all cleaned Parquet files for a specific category (eg. Magazine_Subscriptions) into a single unified Parquet file. The code iterates through the chunked Parquet files, ensures schema consistency, and writes them to a final output file using PyArrow for efficient handling. The process verifies the total rows and schema of the combined dataset, ensuring it is ready for downstream tasks like EDA, sentiment analysis, and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb691ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 cleaned Parquet files to combine\n",
      "Processed C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions1.parquet: 19,940 rows\n",
      "Processed C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions2.parquet: 19,916 rows\n",
      "Processed C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions3.parquet: 19,846 rows\n",
      "Processed C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions4.parquet: 11,224 rows\n",
      "\n",
      "ðŸŽ‰ Combining complete! Saved to C:\\Users\\zachr\\final_cleaned_Magazine_Subscriptions.parquet\n",
      "Total rows written: 70,926\n",
      "Final file shape: (70926, 17)\n",
      "Final file columns: ['parent_asin', 'rating', 'text', 'user_id', 'asin', 'timestamp', 'categories', 'main_category', 'helpful_vote', 'verified_purchase', 'title_y', 'average_rating', 'rating_number', 'price', 'brand', 'review_length', 'year']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# Define paths\n",
    "input_dir = Path(\"C:/Users/zachr/Magazine_Subscriptions\")\n",
    "output_file = Path(\"C:/Users/zachr/final_cleaned_Magazine_Subscriptions.parquet\")\n",
    "\n",
    "# Ensure input directory exists\n",
    "if not input_dir.exists():\n",
    "    raise FileNotFoundError(f\"Input directory {input_dir} does not exist\")\n",
    "\n",
    "# Get list of all cleaned Parquet files\n",
    "input_files = sorted(glob.glob(str(input_dir / \"Magazine_Subscriptions*.parquet\")))\n",
    "if not input_files:\n",
    "    raise FileNotFoundError(f\"No files found matching {input_dir / 'Magazine_Subscriptions*.parquet'}\")\n",
    "\n",
    "print(f\"Found {len(input_files)} cleaned Parquet files to combine\")\n",
    "\n",
    "# Initialize ParquetWriter with the schema from the first file\n",
    "first_file = input_files[0]\n",
    "first_table = pq.read_table(first_file)\n",
    "schema = first_table.schema\n",
    "writer = pq.ParquetWriter(output_file, schema, compression='snappy')\n",
    "\n",
    "total_rows_written = 0\n",
    "\n",
    "# Iterate over each input file\n",
    "for file in input_files:\n",
    "    # Read the Parquet file\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Verify schema consistency\n",
    "    if table.schema != schema:\n",
    "        print(f\"Warning: Schema mismatch in {file}. Expected {schema}, got {table.schema}\")\n",
    "    \n",
    "    # Write the table to the output file\n",
    "    writer.write_table(table)\n",
    "    \n",
    "    total_rows_written += table.num_rows\n",
    "    print(f\"Processed {file}: {table.num_rows:,} rows\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del table\n",
    "    gc.collect()\n",
    "\n",
    "# Close the ParquetWriter\n",
    "writer.close()\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Combining complete! Saved to {output_file}\")\n",
    "print(f\"Total rows written: {total_rows_written:,}\")\n",
    "\n",
    "# Verify the final file\n",
    "final_table = pq.read_table(output_file)\n",
    "print(f\"Final file shape: ({final_table.num_rows}, {final_table.num_columns})\")\n",
    "print(f\"Final file columns: {final_table.column_names}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
