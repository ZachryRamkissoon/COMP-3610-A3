{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb2dfa2",
   "metadata": {},
   "source": [
    "# Part 5 - ALS Recommender System (Sampled Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61cdf86",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "This section installs the required libraries: DuckDB for efficient data sampling, Pandas for data manipulation, and implicit for ALS-based recommendation modeling. The scikit-learn package is used for data splitting and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6765cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827a9bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e44c4f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from implicit) (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from implicit) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from implicit) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from implicit) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alindo\\downloads\\assignment3notebook\\.conda\\lib\\site-packages (from tqdm>=4.27->implicit) (0.4.6)\n",
      "Downloading implicit-0.7.2-cp311-cp311-win_amd64.whl (750 kB)\n",
      "   ---------------------------------------- 0.0/750.8 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/750.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 750.8/750.8 kB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: implicit\n",
      "Successfully installed implicit-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install implicit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad4a603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\zachr\\miniconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\zachr\\miniconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\zachr\\miniconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\zachr\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\zachr\\miniconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097950c",
   "metadata": {},
   "source": [
    "## ALS Recommendation System\n",
    "\n",
    "This section implements the ALS recommendation system on a 0.01% sample of the Amazon Reviews dataset:\n",
    "\n",
    "### Data Sampling: DuckDB samples 0.01% of the data from 34 category-specific Parquet files, yielding 51,477 interactions. Users with fewer than 5 reviews in the sample are filtered out, resulting in 6,472 interactions, 728 users, and 5,962 items.\n",
    "\n",
    "### Data Splitting: The sampled data is split into 80% training (5,177 interactions) and 20% testing (1,295 interactions).\n",
    "\n",
    "### Model Training: An ALS model is trained using the implicit library (factors=100, regularization=0.05, iterations=15) on a sparse user-item matrix.\n",
    "\n",
    "### Evaluation: The model achieves an RMSE of 4.4865 on the test set, indicating prediction challenges possibly due to sparse data or the small sample size.\n",
    "\n",
    "### Recommendations: Top-5 product recommendations are generated for 3 random users, showing predicted ratings (e.g., User AHKLCGILATXKYSYAZROMECV4DPLQ recommends Product B07TSG87YD with a score of 0.361).\n",
    "\n",
    "The process completes in ~102 seconds, operating on a significantly reduced dataset for computational feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224fc399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting ALS ---\n",
      "Operating on approximately 0.0100% sample of the data using DuckDB\n",
      "Scanning for Parquet files: F:/sentiments/sentiments/sentiment_*.parquet\n",
      "Found 34 files. Reading, filtering, and sampling each via DuckDB...\n",
      "Processing file 1/34: sentiment_All_Beauty.parquet...\n",
      "Processing file 2/34: sentiment_Amazon_Fashion.parquet...\n",
      "Processing file 3/34: sentiment_Appliances.parquet...\n",
      "Processing file 4/34: sentiment_Arts_Crafts_and_Sewing.parquet...\n",
      "Processing file 5/34: sentiment_Automotive.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2772eb7ea39a40b289e3fd033dab0d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 6/34: sentiment_Baby_Products.parquet...\n",
      "Processing file 7/34: sentiment_Beauty_and_Personal_Care.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6e16b9e83549ddb09da38ad03f1158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 8/34: sentiment_Books.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049b81d198f544f5b74b5d2c3c66d805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 9/34: sentiment_CDs_and_Vinyl.parquet...\n",
      "Processing file 10/34: sentiment_Cell_Phones_and_Accessories.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e73a00e3d2542d080f0cf998cb3ae81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 11/34: sentiment_Clothing_Shoes_and_Jewelry.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9866a42594b647a4938fa1dada690785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 12/34: sentiment_Digital_Music.parquet...\n",
      "Processing file 13/34: sentiment_Electronics.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc885a8bb9a74c5b8bc3ccec1f173d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 14/34: sentiment_Gift_Cards.parquet...\n",
      "Processing file 15/34: sentiment_Grocery_and_Gourmet_Food.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d89ecb41b87490bb8d95dc9564dd8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 16/34: sentiment_Handmade_Products.parquet...\n",
      "Processing file 17/34: sentiment_Health_and_Household.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f108d6817d4406944343258e4c4f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 18/34: sentiment_Health_and_Personal_Care.parquet...\n",
      "Processing file 19/34: sentiment_Home_and_Kitchen.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40640cc3f1bc48fb9fdb5559b86806d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 20/34: sentiment_Industrial_and_Scientific.parquet...\n",
      "Processing file 21/34: sentiment_Kindle_Store.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be04c9ab13f437ab2a3800ddb759f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 22/34: sentiment_Magazine_Subscriptions.parquet...\n",
      "Processing file 23/34: sentiment_Movies_and_TV.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ed77777735415f96821a842f74f8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 24/34: sentiment_Musical_Instruments.parquet...\n",
      "Processing file 25/34: sentiment_Office_Products.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205ff6c47b9d49308dbb8d91a8091479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 26/34: sentiment_Patio_Lawn_and_Garden.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5577720312e44aba6f204bb547d6596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 27/34: sentiment_Pet_Supplies.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2dbc84f2d44a639524f181984d24b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 28/34: sentiment_Software.parquet...\n",
      "Processing file 29/34: sentiment_Sports_and_Outdoors.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad31d4d17d8249f0a65e56c36b332fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 30/34: sentiment_Subscription_Boxes.parquet...\n",
      "Processing file 31/34: sentiment_Tools_and_Home_Improvement.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c1cff5a95a45dbb4278540afcc2ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 32/34: sentiment_Toys_and_Games.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546559fb40ed4c4bb584bfc4eb422bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 33/34: sentiment_Unknown.parquet...\n",
      "Processing file 34/34: sentiment_Video_Games.parquet...\n",
      "Finished processing 34 files. 0 errors encountered.\n",
      "Concatenating sampled data chunks...\n",
      "Total sampled interactions collected: 51477\n",
      "Closing DuckDB connection.\n",
      "Filtering users with >= 5 reviews *within the collected sample*...\n",
      "Final data after sampling and filtering: 6472 interactions from 728 users and 5962 items.\n",
      "Creating user and item ID mappings...\n",
      "Mapped to 728 unique user indices and 5962 unique item indices.\n",
      "Splitting sampled data into training (80%) and testing (20%)...\n",
      "Training set size (sampled): 5177\n",
      "Test set size (sampled): 1295\n",
      "Creating sparse user-item matrix for training...\n",
      "Sparse matrix created.\n",
      "Training Implicit ALS model (factors=100, regularization=0.05, iterations=15)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alindo\\Downloads\\assignment3notebook\\.conda\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eed169844a7443abb80c163278e8c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model training complete.\n",
      "Evaluating model on the test set sample (calculating RMSE)...\n",
      "Evaluation complete. Test Set RMSE (on sample): 4.4865\n",
      "\n",
      "--- Generating Top 5 Recommendations for 3 Random Users (from sample) ---\n",
      "Selected random users for demo (original IDs from sample): ['AHKLCGILATXKYSYAZROMECV4DPLQ', 'AGBEWUAJTLUIA3RXP52JCFCD267A', 'AE4AYOESICGVKF2I3WA6WAU5SBRQ']\n",
      "\n",
      "Recommendations for User: AHKLCGILATXKYSYAZROMECV4DPLQ (Index: 640)\n",
      "  Top 5 recommendations:\n",
      "    1. Product ID: B07TSG87YD, Predicted Rating: 0.361 (Score: 0.361)\n",
      "    2. Product ID: B0C3LV4FVT, Predicted Rating: 0.358 (Score: 0.358)\n",
      "    3. Product ID: B0C3FG4HSV, Predicted Rating: 0.226 (Score: 0.226)\n",
      "    4. Product ID: B00B7M7CLA, Predicted Rating: 0.218 (Score: 0.218)\n",
      "    5. Product ID: B004GCJMGG, Predicted Rating: 0.218 (Score: 0.218)\n",
      "\n",
      "Recommendations for User: AGBEWUAJTLUIA3RXP52JCFCD267A (Index: 412)\n",
      "  Top 5 recommendations:\n",
      "    1. Product ID: B0BNQGJTF7, Predicted Rating: 0.302 (Score: 0.302)\n",
      "    2. Product ID: B07CMDY1P4, Predicted Rating: 0.202 (Score: 0.202)\n",
      "    3. Product ID: B08ZG4YJQ8, Predicted Rating: 0.202 (Score: 0.202)\n",
      "    4. Product ID: B0CDMGMGFY, Predicted Rating: 0.200 (Score: 0.200)\n",
      "    5. Product ID: B00005TSMN, Predicted Rating: 0.190 (Score: 0.190)\n",
      "\n",
      "Recommendations for User: AE4AYOESICGVKF2I3WA6WAU5SBRQ (Index: 6)\n",
      "  Top 5 recommendations:\n",
      "    1. Product ID: B078T9FSS8, Predicted Rating: 0.475 (Score: 0.475)\n",
      "    2. Product ID: B079TG42L6, Predicted Rating: 0.475 (Score: 0.475)\n",
      "    3. Product ID: B00Q2XEFQO, Predicted Rating: 0.475 (Score: 0.475)\n",
      "    4. Product ID: B09FPTHH95, Predicted Rating: 0.475 (Score: 0.475)\n",
      "    5. Product ID: B0875YJLRM, Predicted Rating: 0.475 (Score: 0.475)\n",
      "\n",
      "--- Script finished in 102.19 seconds ---\n",
      "--- Results based on approximately 0.0100% sample of the original data ---\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import implicit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "input_path_pattern = \"F:/sentiments/sentiments/sentiment_*.parquet\"\n",
    "min_reviews_per_user = 5\n",
    "test_size = 0.20\n",
    "n_recommendations = 5\n",
    "n_demo_users = 3\n",
    "random_state = 42\n",
    "\n",
    "sample_fraction = 0.0001\n",
    "duckdb_sample_percent = sample_fraction * 100\n",
    "\n",
    "als_factors = 100\n",
    "als_regularization = 0.05\n",
    "als_iterations = 15\n",
    "\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--- Starting ALS ---\")\n",
    "print(f\"Operating on approximately {sample_fraction*100:.4f}% sample of the data using DuckDB\")\n",
    "\n",
    "\n",
    "print(f\"Scanning for Parquet files: {input_path_pattern}\")\n",
    "parquet_files = glob.glob(input_path_pattern)\n",
    "\n",
    "if not parquet_files:\n",
    "    print(f\"Error: No files found matching pattern '{input_path_pattern}'. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(parquet_files)} files. Reading, filtering, and sampling each via DuckDB...\")\n",
    "sampled_dfs = []\n",
    "total_rows_processed = 0 # Note: Can't easily get rows processed without extra query per file\n",
    "errors_reading = 0\n",
    "duckdb_con = None\n",
    "\n",
    "try:\n",
    "    duckdb_con = duckdb.connect(config={'memory_limit': '10GB'}) # In-memory, adjust limit if needed\n",
    "\n",
    "    for i, file_path in enumerate(parquet_files):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"Processing file {i+1}/{len(parquet_files)}: {file_name}...\")\n",
    "        try:\n",
    "            # Replace backslashes for SQL compatibility if needed, though DuckDB often handles it\n",
    "            sql_safe_file_path = file_path.replace('\\\\', '/')\n",
    "\n",
    "            query = f\"\"\"\n",
    "            SELECT\n",
    "                user_id,\n",
    "                parent_asin AS product_id,\n",
    "                CAST(rating AS FLOAT) AS rating\n",
    "            FROM read_parquet('{sql_safe_file_path}')\n",
    "            WHERE user_id IS NOT NULL AND TRIM(user_id) != ''\n",
    "              AND parent_asin IS NOT NULL AND TRIM(parent_asin) != ''\n",
    "              AND rating BETWEEN 1 AND 5\n",
    "            USING SAMPLE {duckdb_sample_percent} PERCENT (SYSTEM);\n",
    "            \"\"\"\n",
    "\n",
    "            df_sampled_chunk = duckdb_con.execute(query).fetch_df()\n",
    "\n",
    "            if len(df_sampled_chunk) > 0:\n",
    "                sampled_dfs.append(df_sampled_chunk)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error processing file {file_name}: {e}\")\n",
    "            errors_reading += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"Finished processing {len(parquet_files)} files. {errors_reading} errors encountered.\")\n",
    "\n",
    "    if not sampled_dfs:\n",
    "        print(\"Error: No data was successfully sampled. Check file contents, filters, or sample fraction. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"Concatenating sampled data chunks...\")\n",
    "    interactions_df_sampled = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    n_sampled_interactions = len(interactions_df_sampled)\n",
    "    print(f\"Total sampled interactions collected: {n_sampled_interactions}\")\n",
    "\n",
    "    if n_sampled_interactions == 0:\n",
    "        print(\"Sample is empty after concatenation. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during DuckDB processing loop or concatenation: {e}\")\n",
    "    exit()\n",
    "finally:\n",
    "    if duckdb_con:\n",
    "        print(\"Closing DuckDB connection.\")\n",
    "        duckdb_con.close()\n",
    "\n",
    "\n",
    "print(f\"Filtering users with >= {min_reviews_per_user} reviews *within the collected sample*...\")\n",
    "try:\n",
    "    user_counts = interactions_df_sampled.groupby('user_id')['rating'].transform('count')\n",
    "    interactions_df_final = interactions_df_sampled[user_counts >= min_reviews_per_user].copy() # Use copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    n_interactions_final = len(interactions_df_final)\n",
    "\n",
    "    if n_interactions_final == 0:\n",
    "        print(\"No interaction data remaining after filtering the sample by user review count. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    n_users_final = interactions_df_final[\"user_id\"].nunique()\n",
    "    n_items_final = interactions_df_final[\"product_id\"].nunique()\n",
    "    print(f\"Final data after sampling and filtering: {n_interactions_final} interactions from {n_users_final} users and {n_items_final} items.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during user filtering on the sample: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(\"Creating user and item ID mappings...\")\n",
    "try:\n",
    "    # interactions_df_final is already a Pandas DataFrame\n",
    "    interactions_pd = interactions_df_final\n",
    "\n",
    "    interactions_pd['user_idx'] = interactions_pd['user_id'].astype('category').cat.codes\n",
    "    interactions_pd['item_idx'] = interactions_pd['product_id'].astype('category').cat.codes\n",
    "\n",
    "    user_map = interactions_pd[['user_idx', 'user_id']].drop_duplicates().set_index('user_idx')\n",
    "    item_map = interactions_pd[['item_idx', 'product_id']].drop_duplicates().set_index('item_idx')\n",
    "\n",
    "    n_users = interactions_pd['user_idx'].max() + 1\n",
    "    n_items = interactions_pd['item_idx'].max() + 1\n",
    "    print(f\"Mapped to {n_users} unique user indices and {n_items} unique item indices.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating ID mappings: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(f\"Splitting sampled data into training ({1-test_size:.0%}) and testing ({test_size:.0%})...\")\n",
    "try:\n",
    "    train_pd, test_pd = train_test_split(\n",
    "        interactions_pd,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=interactions_pd['user_idx']\n",
    "    )\n",
    "    print(f\"Training set size (sampled): {len(train_pd)}\")\n",
    "    print(f\"Test set size (sampled): {len(test_pd)}\")\n",
    "\n",
    "    if train_pd.empty or test_pd.empty:\n",
    "         print(\"Train or test set is empty after split. Check sample or split ratio.\")\n",
    "         exit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting sampled data: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(\"Creating sparse user-item matrix for training...\")\n",
    "try:\n",
    "    train_user_items = csr_matrix(\n",
    "        (train_pd['rating'].astype(np.float32),\n",
    "         (train_pd['user_idx'], train_pd['item_idx'])),\n",
    "        shape=(n_users, n_items)\n",
    "    )\n",
    "    print(\"Sparse matrix created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating sparse matrix: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(f\"Training Implicit ALS model (factors={als_factors}, regularization={als_regularization}, iterations={als_iterations})...\")\n",
    "try:\n",
    "    model = implicit.als.AlternatingLeastSquares(\n",
    "        factors=als_factors,\n",
    "        regularization=als_regularization,\n",
    "        iterations=als_iterations,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(train_user_items)\n",
    "    print(\"ALS model training complete.\")\n",
    "    user_factors = model.user_factors\n",
    "    item_factors = model.item_factors\n",
    "except Exception as e:\n",
    "    print(f\"Error training ALS model: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(\"Evaluating model on the test set sample (calculating RMSE)...\")\n",
    "try:\n",
    "    test_user_indices = test_pd['user_idx'].values\n",
    "    test_item_indices = test_pd['item_idx'].values\n",
    "    actual_ratings = test_pd['rating'].values\n",
    "\n",
    "    predicted_ratings = []\n",
    "    for u_idx, i_idx in zip(test_user_indices, test_item_indices):\n",
    "        if u_idx < user_factors.shape[0] and i_idx < item_factors.shape[0]:\n",
    "            pred = user_factors[u_idx, :].dot(item_factors[i_idx, :])\n",
    "            predicted_ratings.append(pred)\n",
    "        else:\n",
    "             predicted_ratings.append(np.nan)\n",
    "\n",
    "    valid_indices = ~np.isnan(predicted_ratings)\n",
    "    if not np.all(valid_indices):\n",
    "        print(f\"Warning: {np.sum(~valid_indices)} test interactions could not be predicted (index out of bounds?).\")\n",
    "        actual_ratings = actual_ratings[valid_indices]\n",
    "        predicted_ratings = np.array(predicted_ratings)[valid_indices]\n",
    "\n",
    "    if len(predicted_ratings) > 0:\n",
    "        mse = mean_squared_error(actual_ratings, predicted_ratings)\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f\"Evaluation complete. Test Set RMSE (on sample): {rmse:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid predictions generated for the test set. Cannot calculate RMSE.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating model: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(f\"\\n--- Generating Top {n_recommendations} Recommendations for {n_demo_users} Random Users (from sample) ---\")\n",
    "try:\n",
    "    all_train_user_indices = train_pd['user_idx'].unique()\n",
    "\n",
    "    if len(all_train_user_indices) == 0:\n",
    "         print(\"No users found in the training set sample to generate recommendations for.\")\n",
    "    else:\n",
    "        num_users_to_sample = min(n_demo_users, len(all_train_user_indices))\n",
    "        if num_users_to_sample < n_demo_users:\n",
    "            print(f\"Warning: Only {num_users_to_sample} unique users in training data sample. Showing recommendations for {num_users_to_sample}.\")\n",
    "\n",
    "\n",
    "        random_user_indices = random.sample(list(all_train_user_indices), num_users_to_sample)\n",
    "        random_users_original_ids = [user_map.loc[idx, 'user_id'] for idx in random_user_indices]\n",
    "        print(f\"Selected random users for demo (original IDs from sample): {random_users_original_ids}\")\n",
    "\n",
    "        for user_idx in random_user_indices:\n",
    "            original_user_id = user_map.loc[user_idx, 'user_id']\n",
    "            print(f\"\\nRecommendations for User: {original_user_id} (Index: {user_idx})\")\n",
    "\n",
    "            recommended_indices, scores = model.recommend(\n",
    "                user_idx,\n",
    "                train_user_items[user_idx],\n",
    "                N=n_recommendations,\n",
    "                filter_already_liked_items=True\n",
    "            )\n",
    "\n",
    "            if len(recommended_indices) == 0:\n",
    "                print(\"  No recommendations could be generated.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Top {len(recommended_indices)} recommendations:\")\n",
    "            for i, item_idx in enumerate(recommended_indices):\n",
    "                 if item_idx < item_factors.shape[0]:\n",
    "                    original_product_id = item_map.loc[item_idx, 'product_id']\n",
    "                    predicted_rating = user_factors[user_idx, :].dot(item_factors[item_idx, :])\n",
    "                    print(f\"    {i+1}. Product ID: {original_product_id}, Predicted Rating: {predicted_rating:.3f} (Score: {scores[i]:.3f})\")\n",
    "                 else:\n",
    "                     print(f\"    {i+1}. Recommended item index {item_idx} out of bounds.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating recommendations: {e}\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- Script finished in {end_time - start_time:.2f} seconds ---\")\n",
    "print(f\"--- Results based on approximately {sample_fraction*100:.4f}% sample of the original data ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
