{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33b22d5",
   "metadata": {},
   "source": [
    "# Part 2 - Data Cleaning & Preprocessing - cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2960de3",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e67124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26615eb",
   "metadata": {},
   "source": [
    "# Cleaning in Batches\n",
    "This section processes the merged dataset for a specific category (eg. Magazine_Subscriptions) in chunks to handle the large-scale dataset efficiently. The code loads the input Parquet file, filters out columns, and applies cleaning steps including dropping invalid/missing ratings and empty reviews, extracting brand information, removing duplicates, and deriving new columns (review length and year). Each cleaned chunk is saved as a separate Parquet file to manage memory efficiently since the dataset included some large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing files in C:\\Users\\zachr\\Magazine_Subscriptions\n",
      "âœ… Processed and saved chunk 1: 19,940 rows to C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions1.parquet\n",
      "âœ… Processed and saved chunk 2: 19,916 rows to C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions2.parquet\n",
      "âœ… Processed and saved chunk 3: 19,846 rows to C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions3.parquet\n",
      "âœ… Processed and saved chunk 4: 11,224 rows to C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions4.parquet\n",
      "\n",
      " Cleaning complete! 4 files saved to C:\\Users\\zachr\\Magazine_Subscriptions\n",
      "Total rows processed: 70,926\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "input_file = Path(\"C:/Users/zachr/Downloads/merged_Magazine_Subscriptions.parquet\")\n",
    "output_dir = Path(\"C:/Users/zachr/Magazine_Subscriptions\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if input file exists\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"Input file {input_file} does not exist\")\n",
    "\n",
    "# Clear existing cleaned files to start fresh\n",
    "for file in output_dir.glob(\"cleaned__Magazine_Subscriptions*.parquet\"):\n",
    "    os.remove(file)\n",
    "print(f\"Cleared existing files in {output_dir}\")\n",
    "\n",
    "# Create a dataset for streaming\n",
    "dataset = ds.dataset(input_file, format=\"parquet\")\n",
    "\n",
    "# Specify columns to keep\n",
    "columns_to_keep = [\n",
    "    \"parent_asin\", \"rating\", \"text\", \"details\", \"store\", \"user_id\", \"asin\", \n",
    "    \"timestamp\", \"categories\", \"main_category\", \"helpful_vote\", \n",
    "    \"verified_purchase\", \"title_y\", \"average_rating\", \"rating_number\", \"price\"\n",
    "]\n",
    "\n",
    "# Scanner for chunked reading\n",
    "scanner = dataset.scanner(columns=columns_to_keep, batch_size=50_000)\n",
    "\n",
    "# Define brand extraction logic\n",
    "def extract_brand(details, store):\n",
    "    if isinstance(details, dict):\n",
    "        brand = details.get(\"brand\")\n",
    "        if brand:\n",
    "            return brand.strip()\n",
    "    if isinstance(store, str) and store.strip():\n",
    "        return store.strip()\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Process in chunks\n",
    "total_rows_processed = 0\n",
    "chunk_index = 1\n",
    "\n",
    "for i, batch in enumerate(scanner.to_batches()):\n",
    "    # Convert batch to Pandas DataFrame\n",
    "    df = batch.to_pandas()\n",
    "\n",
    "    # Drop invalid/missing ratings\n",
    "    df = df[df[\"rating\"].isin([1, 2, 3, 4, 5])]\n",
    "\n",
    "    # Drop empty/null review texts\n",
    "    df = df[df[\"text\"].notnull() & (df[\"text\"].str.strip() != \"\")]\n",
    "\n",
    "    # Extract brand\n",
    "    df[\"brand\"] = df.apply(lambda row: extract_brand(row[\"details\"], row[\"store\"]), axis=1)\n",
    "\n",
    "    # Drop details/store after extracting brand\n",
    "    df.drop(columns=[\"details\", \"store\"], inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(subset=[\"user_id\", \"asin\", \"text\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    # Derived column: review length\n",
    "    df[\"review_length\"] = df[\"text\"].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "    # Derived column: year from timestamp\n",
    "    df[\"year\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", errors=\"coerce\").dt.year\n",
    "\n",
    "    # Write cleaned chunk to a separate Parquet file\n",
    "    output_file = output_dir / f\"Magazine_Subscriptions{chunk_index}.parquet\"\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    pq.write_table(table, output_file, compression='snappy')\n",
    "    \n",
    "    total_rows_processed += len(df)\n",
    "    print(f\"âœ… Processed and saved chunk {chunk_index}: {len(df):,} rows to {output_file}\")\n",
    "\n",
    "    # Increment chunk index\n",
    "    chunk_index += 1\n",
    "    \n",
    "    # Clean up memory\n",
    "    del df\n",
    "    del table\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n Cleaning complete! {chunk_index - 1} files saved to {output_dir}\")\n",
    "print(f\"Total rows processed: {total_rows_processed:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43983e18",
   "metadata": {},
   "source": [
    "# Combining Cleaned Parquet Files\n",
    "This section combines all cleaned Parquet files for a specific category (eg. Magazine_Subscriptions) into a single unified Parquet file. The code iterates through the chunked Parquet files, ensures schema consistency, and writes them to a final output file using PyArrow for efficient handling. The process verifies the total rows and schema of the combined dataset, ensuring it is ready for downstream tasks like EDA, sentiment analysis, and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb691ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 cleaned Parquet files to combine\n",
      "Processed C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions1.parquet: 19,940 rows\n",
      "Processed C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions2.parquet: 19,916 rows\n",
      "Processed C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions3.parquet: 19,846 rows\n",
      "Processed C:\\Users\\zachr\\Magazine_Subscriptions\\Magazine_Subscriptions4.parquet: 11,224 rows\n",
      "\n",
      "ðŸŽ‰ Combining complete! Saved to C:\\Users\\zachr\\final_cleaned_Magazine_Subscriptions.parquet\n",
      "Total rows written: 70,926\n",
      "Final file shape: (70926, 17)\n",
      "Final file columns: ['parent_asin', 'rating', 'text', 'user_id', 'asin', 'timestamp', 'categories', 'main_category', 'helpful_vote', 'verified_purchase', 'title_y', 'average_rating', 'rating_number', 'price', 'brand', 'review_length', 'year']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# Define paths\n",
    "input_dir = Path(\"C:/Users/zachr/Magazine_Subscriptions\")\n",
    "output_file = Path(\"C:/Users/zachr/final_cleaned_Magazine_Subscriptions.parquet\")\n",
    "\n",
    "# Ensure input directory exists\n",
    "if not input_dir.exists():\n",
    "    raise FileNotFoundError(f\"Input directory {input_dir} does not exist\")\n",
    "\n",
    "# Get list of all cleaned Parquet files\n",
    "input_files = sorted(glob.glob(str(input_dir / \"Magazine_Subscriptions*.parquet\")))\n",
    "if not input_files:\n",
    "    raise FileNotFoundError(f\"No files found matching {input_dir / 'Magazine_Subscriptions*.parquet'}\")\n",
    "\n",
    "print(f\"Found {len(input_files)} cleaned Parquet files to combine\")\n",
    "\n",
    "# Initialize ParquetWriter with the schema from the first file\n",
    "first_file = input_files[0]\n",
    "first_table = pq.read_table(first_file)\n",
    "schema = first_table.schema\n",
    "writer = pq.ParquetWriter(output_file, schema, compression='snappy')\n",
    "\n",
    "total_rows_written = 0\n",
    "\n",
    "# Iterate over each input file\n",
    "for file in input_files:\n",
    "    # Read the Parquet file\n",
    "    table = pq.read_table(file)\n",
    "    \n",
    "    # Verify schema consistency\n",
    "    if table.schema != schema:\n",
    "        print(f\"Warning: Schema mismatch in {file}. Expected {schema}, got {table.schema}\")\n",
    "    \n",
    "    # Write the table to the output file\n",
    "    writer.write_table(table)\n",
    "    \n",
    "    total_rows_written += table.num_rows\n",
    "    print(f\"Processed {file}: {table.num_rows:,} rows\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del table\n",
    "    gc.collect()\n",
    "\n",
    "# Close the ParquetWriter\n",
    "writer.close()\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Combining complete! Saved to {output_file}\")\n",
    "print(f\"Total rows written: {total_rows_written:,}\")\n",
    "\n",
    "# Verify the final file\n",
    "final_table = pq.read_table(output_file)\n",
    "print(f\"Final file shape: ({final_table.num_rows}, {final_table.num_columns})\")\n",
    "print(f\"Final file columns: {final_table.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb6f91",
   "metadata": {},
   "source": [
    "# Verification Check\n",
    "Check to ensure all cleaning steps were applied accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fe5e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>asin</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>categories</th>\n",
       "      <th>main_category</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>title_y</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>review_length</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00HLSSQKK</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Wonderful recipes in this magazine.</td>\n",
       "      <td>AE7Y5RLYIKHOZB5NKKOEKYG2SPSQ</td>\n",
       "      <td>B00HLSSQKK</td>\n",
       "      <td>1608046142433</td>\n",
       "      <td>['Magazine Subscriptions' 'Cooking, Food &amp; Win...</td>\n",
       "      <td>Magazine Subscriptions</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Cooking With Paula Deen</td>\n",
       "      <td>4.6</td>\n",
       "      <td>538</td>\n",
       "      <td>None</td>\n",
       "      <td>Hoffman Media Inc</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B002PXW04Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great sports magazine that's on my 9 year olds...</td>\n",
       "      <td>AHORTSSMI6ZZFUFWMPT4UFR2ISUQ</td>\n",
       "      <td>B002PXW04Y</td>\n",
       "      <td>1470972512000</td>\n",
       "      <td>['Magazine Subscriptions' 'Sports, Recreation ...</td>\n",
       "      <td>Magazine Subscriptions</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Sports Illustrated KIDS    Print Magazine</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2363</td>\n",
       "      <td>None</td>\n",
       "      <td>Maven</td>\n",
       "      <td>11</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00HCR5090</td>\n",
       "      <td>5.0</td>\n",
       "      <td>\"Joy of Kosher\" magazine fills a much-needed n...</td>\n",
       "      <td>AFW2PDT3AMT4X3PYQG7FJZH5FXFA</td>\n",
       "      <td>B00HCR5090</td>\n",
       "      <td>1396827173000</td>\n",
       "      <td>['Magazine Subscriptions' 'Cooking, Food &amp; Win...</td>\n",
       "      <td>Magazine Subscriptions</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>Joy of Kosher With Jamie Geller    Print Magazine</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Kosher Media Network Llc</td>\n",
       "      <td>558</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000ARXXS</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've been addicted to Martha Stewart's Everyda...</td>\n",
       "      <td>AFW2PDT3AMT4X3PYQG7FJZH5FXFA</td>\n",
       "      <td>B0000ARXXS</td>\n",
       "      <td>1181880255000</td>\n",
       "      <td>[]</td>\n",
       "      <td>Magazine Subscriptions</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>Everyday Food</td>\n",
       "      <td>3.6</td>\n",
       "      <td>52</td>\n",
       "      <td>None</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>338</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0025ZOVEO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Too many ads!</td>\n",
       "      <td>AFUB7CHTXRPD447QVQCHBZVN2IPQ</td>\n",
       "      <td>B0025ZOVEO</td>\n",
       "      <td>1591046680246</td>\n",
       "      <td>['Magazine Subscriptions' 'Sports, Recreation ...</td>\n",
       "      <td>Magazine Subscriptions</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>Outside    Print Magazine</td>\n",
       "      <td>4.3</td>\n",
       "      <td>355</td>\n",
       "      <td>None</td>\n",
       "      <td>Outside</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parent_asin  rating                                               text  \\\n",
       "0  B00HLSSQKK     5.0                Wonderful recipes in this magazine.   \n",
       "1  B002PXW04Y     4.0  Great sports magazine that's on my 9 year olds...   \n",
       "2  B00HCR5090     5.0  \"Joy of Kosher\" magazine fills a much-needed n...   \n",
       "3  B0000ARXXS     5.0  I've been addicted to Martha Stewart's Everyda...   \n",
       "4  B0025ZOVEO     1.0                                      Too many ads!   \n",
       "\n",
       "                        user_id        asin      timestamp  \\\n",
       "0  AE7Y5RLYIKHOZB5NKKOEKYG2SPSQ  B00HLSSQKK  1608046142433   \n",
       "1  AHORTSSMI6ZZFUFWMPT4UFR2ISUQ  B002PXW04Y  1470972512000   \n",
       "2  AFW2PDT3AMT4X3PYQG7FJZH5FXFA  B00HCR5090  1396827173000   \n",
       "3  AFW2PDT3AMT4X3PYQG7FJZH5FXFA  B0000ARXXS  1181880255000   \n",
       "4  AFUB7CHTXRPD447QVQCHBZVN2IPQ  B0025ZOVEO  1591046680246   \n",
       "\n",
       "                                          categories           main_category  \\\n",
       "0  ['Magazine Subscriptions' 'Cooking, Food & Win...  Magazine Subscriptions   \n",
       "1  ['Magazine Subscriptions' 'Sports, Recreation ...  Magazine Subscriptions   \n",
       "2  ['Magazine Subscriptions' 'Cooking, Food & Win...  Magazine Subscriptions   \n",
       "3                                                 []  Magazine Subscriptions   \n",
       "4  ['Magazine Subscriptions' 'Sports, Recreation ...  Magazine Subscriptions   \n",
       "\n",
       "   helpful_vote  verified_purchase  \\\n",
       "0             0               True   \n",
       "1             1               True   \n",
       "2             4              False   \n",
       "3             4              False   \n",
       "4            13               True   \n",
       "\n",
       "                                             title_y  average_rating  \\\n",
       "0                            Cooking With Paula Deen             4.6   \n",
       "1          Sports Illustrated KIDS    Print Magazine             4.3   \n",
       "2  Joy of Kosher With Jamie Geller    Print Magazine             5.0   \n",
       "3                                      Everyday Food             3.6   \n",
       "4                          Outside    Print Magazine             4.3   \n",
       "\n",
       "   rating_number price                     brand  review_length  year  \n",
       "0            538  None         Hoffman Media Inc              5  2020  \n",
       "1           2363  None                     Maven             11  2016  \n",
       "2              5  None  Kosher Media Network Llc            558  2014  \n",
       "3             52  None                   Unknown            338  2007  \n",
       "4            355  None                   Outside              3  2020  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"C:/Users/zachr/final_cleaned_Magazine_Subscriptions.parquet\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
